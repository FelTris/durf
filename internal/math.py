# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python3
"""Mathy utility functions."""
import jax
import jax.numpy as jnp
import jax.scipy as jsp


def matmul(a, b):
    """jnp.matmul defaults to bfloat16, but this helper function doesn't."""
    return jnp.matmul(a, b, precision=jax.lax.Precision.HIGHEST)


def safe_norm(x):
    """jnp.linalg.norm is not differentiable at zero. This function gets around this, but introduces uncertainties.
        Only use for contraction in mip360, where it shouldn't cause issues."""
    norm_sqs = jnp.sum(x**2, axis=-1, keepdims=True)
    norm_save = jnp.where(norm_sqs < 1e-12, 1e-12, norm_sqs)
    return jnp.sqrt(norm_save)


def safe_trig_helper(x, fn, t=100 * jnp.pi):
    return fn(jnp.where(jnp.abs(x) < t, x, x % t))


def safe_cos(x):
    """jnp.cos() on a TPU may NaN out for large values."""
    return safe_trig_helper(x, jnp.cos)


def safe_sin(x):
    """jnp.sin() on a TPU may NaN out for large values."""
    return safe_trig_helper(x, jnp.sin)


def mse_to_psnr(mse):
    """Compute PSNR given an MSE (we assume the maximum pixel value is 1)."""
    return -10. / jnp.log(10.) * jnp.log(mse)


def psnr_to_mse(psnr):
    """Compute MSE given a PSNR (we assume the maximum pixel value is 1)."""
    return jnp.exp(-0.1 * jnp.log(10.) * psnr)


def compute_avg_error(psnr, ssim, lpips):
    """The 'average' error used in the paper."""
    mse = psnr_to_mse(psnr)
    dssim = jnp.sqrt(1 - ssim)
    return jnp.exp(jnp.mean(jnp.log(jnp.array([mse, dssim, lpips]))))


def compute_ssim(img0,
                 img1,
                 max_val,
                 filter_size=11,
                 filter_sigma=1.5,
                 k1=0.01,
                 k2=0.03,
                 return_map=False):
    """Computes SSIM from two images.

    This function was modeled after tf.image.ssim, and should produce comparable
    output.

    Args:
      img0: array. An image of size [..., width, height, num_channels].
      img1: array. An image of size [..., width, height, num_channels].
      max_val: float > 0. The maximum magnitude that `img0` or `img1` can have.
      filter_size: int >= 1. Window size.
      filter_sigma: float > 0. The bandwidth of the Gaussian used for filtering.
      k1: float > 0. One of the SSIM dampening parameters.
      k2: float > 0. One of the SSIM dampening parameters.
      return_map: Bool. If True, will cause the per-pixel SSIM "map" to returned

    Returns:
      Each image's mean SSIM, or a tensor of individual values if `return_map`.
    """
    # Construct a 1D Gaussian blur filter.
    hw = filter_size // 2
    shift = (2 * hw - filter_size + 1) / 2
    f_i = ((jnp.arange(filter_size) - hw + shift) / filter_sigma) ** 2
    filt = jnp.exp(-0.5 * f_i)
    filt /= jnp.sum(filt)

    # Blur in x and y (faster than the 2D convolution).
    def convolve2d(z, f):
        return jsp.signal.convolve2d(
            z, f, mode='valid', precision=jax.lax.Precision.HIGHEST)

    filt_fn1 = lambda z: convolve2d(z, filt[:, None])
    filt_fn2 = lambda z: convolve2d(z, filt[None, :])

    # Vmap the blurs to the tensor size, and then compose them.
    num_dims = len(img0.shape)
    map_axes = tuple(list(range(num_dims - 3)) + [num_dims - 1])
    for d in map_axes:
        filt_fn1 = jax.vmap(filt_fn1, in_axes=d, out_axes=d)
        filt_fn2 = jax.vmap(filt_fn2, in_axes=d, out_axes=d)
    filt_fn = lambda z: filt_fn1(filt_fn2(z))

    mu0 = filt_fn(img0)
    mu1 = filt_fn(img1)
    mu00 = mu0 * mu0
    mu11 = mu1 * mu1
    mu01 = mu0 * mu1
    sigma00 = filt_fn(img0 ** 2) - mu00
    sigma11 = filt_fn(img1 ** 2) - mu11
    sigma01 = filt_fn(img0 * img1) - mu01

    # Clip the variances and covariances to valid values.
    # Variance must be non-negative:
    sigma00 = jnp.maximum(0., sigma00)
    sigma11 = jnp.maximum(0., sigma11)
    sigma01 = jnp.sign(sigma01) * jnp.minimum(
        jnp.sqrt(sigma00 * sigma11), jnp.abs(sigma01))

    c1 = (k1 * max_val) ** 2
    c2 = (k2 * max_val) ** 2
    numer = (2 * mu01 + c1) * (2 * sigma01 + c2)
    denom = (mu00 + mu11 + c1) * (sigma00 + sigma11 + c2)
    ssim_map = numer / denom
    ssim = jnp.mean(ssim_map, list(range(num_dims - 3, num_dims)))
    return ssim_map if return_map else ssim


def linear_to_srgb(linear):
    # Assumes `linear` is in [0, 1]. https://en.wikipedia.org/wiki/SRGB
    eps = jnp.finfo(jnp.float32).eps
    srgb0 = 323 / 25 * linear
    srgb1 = (211 * jnp.maximum(eps, linear) ** (5 / 12) - 11) / 200
    return jnp.where(linear <= 0.0031308, srgb0, srgb1)


def srgb_to_linear(srgb):
    # Assumes `srgb` is in [0, 1]. https://en.wikipedia.org/wiki/SRGB
    eps = jnp.finfo(jnp.float32).eps
    linear0 = 25 / 323 * srgb
    linear1 = jnp.maximum(eps, ((200 * srgb + 11) / (211))) ** (12 / 5)
    return jnp.where(srgb <= 0.04045, linear0, linear1)


def learning_rate_decay(step,
                        lr_init,
                        lr_final,
                        max_steps,
                        lr_delay_steps=0,
                        lr_delay_mult=1):
    """Continuous learning rate decay function.

    The returned rate is lr_init when step=0 and lr_final when step=max_steps, and
    is log-linearly interpolated elsewhere (equivalent to exponential decay).
    If lr_delay_steps>0 then the learning rate will be scaled by some smooth
    function of lr_delay_mult, such that the initial learning rate is
    lr_init*lr_delay_mult at the beginning of optimization but will be eased back
    to the normal learning rate when steps>lr_delay_steps.

    Args:
      step: int, the current optimization step.
      lr_init: float, the initial learning rate.
      lr_final: float, the final learning rate.
      max_steps: int, the number of steps during optimization.
      lr_delay_steps: int, the number of steps to delay the full learning rate.
      lr_delay_mult: float, the multiplier on the rate when delaying it.

    Returns:
      lr: the learning for current step 'step'.
    """
    if lr_delay_steps > 0:
        # A kind of reverse cosine decay.
        delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(
            0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1))
    else:
        delay_rate = 1.
    t = jnp.clip(step / max_steps, 0, 1)
    log_lerp = jnp.exp(jnp.log(lr_init) * (1 - t) + jnp.log(lr_final) * t)
    return delay_rate * log_lerp


def freq_alpha_rate(step,
                    alpha_init,
                    alpha_final,
                    alpha_delay_steps,
                    alpha_max_steps):
    """
    This function will linearly increase alpha from alpha_init at step == delay_steps
    until it reaches alpha_final at steps == max_steps.

    Args:
      step: int, the current optimization step.
      alpha_init: float, the initial freq encoding alpha
      alpha_final: float, max freq encoding
      alpha_delay_steps: int, the number of steps until full pos encoding
      alpha_max_steps: int, the number of steps to delay increasing alpha

    Returns:
      lr: alpha, according to current optimization step
    """

    if step < alpha_delay_steps:
        return alpha_init
    elif step < alpha_max_steps:
        alpha = (step - alpha_delay_steps) / (alpha_max_steps-alpha_delay_steps) * alpha_final
        return alpha
    else:
        return alpha_final


def sorted_piecewise_constant_pdf(key, bins, weights, num_samples, randomized):
    """Piecewise-Constant PDF sampling from sorted bins.

    Args:
      key: jnp.ndarray(float32), [2,], random number generator.
      bins: jnp.ndarray(float32), [batch_size, num_bins + 1].
      weights: jnp.ndarray(float32), [batch_size, num_bins].
      num_samples: int, the number of samples.
      randomized: bool, use randomized samples.

    Returns:
      t_samples: jnp.ndarray(float32), [batch_size, num_samples].
    """
    # Pad each weight vector (only if necessary) to bring its sum to `eps`. This
    # avoids NaNs when the input is zeros or small, but has no effect otherwise.
    eps = 1e-5
    weight_sum = jnp.sum(weights, axis=-1, keepdims=True)
    padding = jnp.maximum(0, eps - weight_sum)
    weights += padding / weights.shape[-1]
    weight_sum += padding

    # Compute the PDF and CDF for each weight vector, while ensuring that the CDF
    # starts with exactly 0 and ends with exactly 1.
    pdf = weights / weight_sum
    cdf = jnp.minimum(1, jnp.cumsum(pdf[..., :-1], axis=-1))
    cdf = jnp.concatenate([
        jnp.zeros(list(cdf.shape[:-1]) + [1]), cdf,
        jnp.ones(list(cdf.shape[:-1]) + [1])
    ],
        axis=-1)

    # Draw uniform samples.
    if randomized:
        s = 1 / num_samples
        u = jnp.arange(num_samples) * s
        u += jax.random.uniform(
            key,
            list(cdf.shape[:-1]) + [num_samples],
            maxval=s - jnp.finfo('float32').eps)
        # `u` is in [0, 1) --- it can be zero, but it can never be 1.
        u = jnp.minimum(u, 1. - jnp.finfo('float32').eps)
    else:
        # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].
        u = jnp.linspace(0., 1. - jnp.finfo('float32').eps, num_samples)
        u = jnp.broadcast_to(u, list(cdf.shape[:-1]) + [num_samples])

    # Identify the location in `cdf` that corresponds to a random sample.
    # The final `True` index in `mask` will be the start of the sampled interval.
    mask = u[..., None, :] >= cdf[..., :, None]

    def find_interval(x):
        # Grab the value where `mask` switches from True to False, and vice versa.
        # This approach takes advantage of the fact that `x` is sorted.
        x0 = jnp.max(jnp.where(mask, x[..., None], x[..., :1, None]), -2)
        x1 = jnp.min(jnp.where(~mask, x[..., None], x[..., -1:, None]), -2)
        return x0, x1

    bins_g0, bins_g1 = find_interval(bins)
    cdf_g0, cdf_g1 = find_interval(cdf)

    t = jnp.clip(jnp.nan_to_num((u - cdf_g0) / (cdf_g1 - cdf_g0), 0), 0, 1)
    samples = bins_g0 + t * (bins_g1 - bins_g0)
    return samples
